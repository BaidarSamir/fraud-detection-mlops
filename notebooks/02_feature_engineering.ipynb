{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93ee279",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection - Feature Engineering\n",
    "\n",
    "## Overview\n",
    "This notebook implements comprehensive feature engineering for the fraud detection model.\n",
    "Good feature engineering is often the difference between a mediocre and excellent fraud detection system.\n",
    "\n",
    "## Feature Engineering Strategy\n",
    "1. **Missing Value Handling**: Intelligent imputation strategies (not just dropping)\n",
    "2. **Temporal Features**: Extract time patterns from TransactionDT\n",
    "3. **Categorical Encoding**: Target encoding for high-cardinality features\n",
    "4. **Interaction Features**: Combinations of card, address, and email features\n",
    "5. **Aggregation Features**: Transaction patterns per card/user\n",
    "6. **Feature Scaling**: Normalize where appropriate\n",
    "\n",
    "## Why These Choices Matter for Fraud Detection\n",
    "- Fraudsters often operate at unusual times (temporal features)\n",
    "- They reuse compromised cards/addresses (aggregation features)\n",
    "- Specific combinations signal fraud (interaction features)\n",
    "- Missing data patterns can indicate fraud attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Define paths\n",
    "BASE_PATH = Path('..').resolve()\n",
    "DATA_PATH = BASE_PATH / 'Data' / 'raw'\n",
    "PROCESSED_PATH = BASE_PATH / 'Data' / 'processed'\n",
    "FEATURES_PATH = BASE_PATH / 'Data' / 'features'\n",
    "OUTPUT_PATH = BASE_PATH / 'outputs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "FEATURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add src to path for importing custom modules\n",
    "sys.path.insert(0, str(BASE_PATH / 'src'))\n",
    "\n",
    "print(f\"Data Path: {DATA_PATH}\")\n",
    "print(f\"Processed Path: {PROCESSED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge data\n",
    "print(\"Loading data...\")\n",
    "train_transaction = pd.read_csv(DATA_PATH / 'train_transaction.csv')\n",
    "train_identity = pd.read_csv(DATA_PATH / 'train_identity.csv')\n",
    "\n",
    "# Merge datasets\n",
    "train_df = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "# Load test data for consistent encoding\n",
    "test_transaction = pd.read_csv(DATA_PATH / 'test_transaction.csv')\n",
    "test_identity = pd.read_csv(DATA_PATH / 'test_identity.csv')\n",
    "test_df = test_transaction.merge(test_identity, on='TransactionID', how='left')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Free memory\n",
    "del train_transaction, train_identity, test_transaction, test_identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb4881",
   "metadata": {},
   "source": [
    "## 1. Memory Optimization\n",
    "\n",
    "Before feature engineering, we optimize memory usage to handle the large dataset efficiently.\n",
    "This is a critical production consideration for Databricks/Spark environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e15846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce memory usage by downcasting numeric types.\n",
    "    This is essential for production systems handling large datasets.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        verbose: print memory reduction stats\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with optimized dtypes\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Downcast integers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                    \n",
    "            # Downcast floats\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB '\n",
    "              f'({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = reduce_memory_usage(train_df)\n",
    "test_df = reduce_memory_usage(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b5857",
   "metadata": {},
   "source": [
    "## 2. Feature Type Classification\n",
    "\n",
    "We classify features into groups for appropriate preprocessing:\n",
    "- **Numerical**: V-features, amounts, counts\n",
    "- **Categorical**: Product codes, card info, device info\n",
    "- **Binary**: Match features (M1-M9)\n",
    "- **Temporal**: TransactionDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2846d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups based on domain knowledge\n",
    "# These groupings are based on the IEEE-CIS feature descriptions\n",
    "\n",
    "# Columns to exclude from features\n",
    "EXCLUDE_COLS = ['TransactionID', 'isFraud']\n",
    "\n",
    "# Categorical columns (object type or known categorical)\n",
    "CATEGORICAL_COLS = [\n",
    "    'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "    'DeviceType', 'DeviceInfo'\n",
    "] + [f'id_{i:02d}' for i in range(12, 39)]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "CATEGORICAL_COLS = [c for c in CATEGORICAL_COLS if c in train_df.columns]\n",
    "\n",
    "# Numerical columns (everything else except ID and target)\n",
    "NUMERICAL_COLS = [c for c in train_df.columns \n",
    "                  if c not in CATEGORICAL_COLS + EXCLUDE_COLS]\n",
    "\n",
    "print(f\"Categorical features: {len(CATEGORICAL_COLS)}\")\n",
    "print(f\"Numerical features: {len(NUMERICAL_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e00075",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis and Handling\n",
    "\n",
    "### Strategy:\n",
    "1. **Drop columns with >90% missing**: These provide little predictive value\n",
    "2. **Numerical features**: Impute with median (robust to outliers in fraud data)\n",
    "3. **Categorical features**: Impute with mode or create 'missing' category\n",
    "4. **Create missing indicators**: The missingness pattern itself can be predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing value patterns\n",
    "missing_pct = train_df.isnull().sum() / len(train_df) * 100\n",
    "\n",
    "# Identify columns to drop (>90% missing)\n",
    "cols_to_drop = missing_pct[missing_pct > 90].index.tolist()\n",
    "print(f\"Columns with >90% missing (to be dropped): {len(cols_to_drop)}\")\n",
    "\n",
    "# Columns with high but not extreme missing (create indicators)\n",
    "cols_high_missing = missing_pct[(missing_pct > 50) & (missing_pct <= 90)].index.tolist()\n",
    "print(f\"Columns with 50-90% missing (create indicators): {len(cols_high_missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12568e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(train_df, test_df, categorical_cols, numerical_cols, \n",
    "                          drop_threshold=0.90, create_indicators=True):\n",
    "    \"\"\"\n",
    "    Comprehensive missing value handling.\n",
    "    \n",
    "    Strategy:\n",
    "    - Drop columns with missing rate > drop_threshold\n",
    "    - Create missing indicators for important columns\n",
    "    - Impute numerical with median (robust to fraud outliers)\n",
    "    - Impute categorical with mode or 'missing' category\n",
    "    \n",
    "    Args:\n",
    "        train_df, test_df: DataFrames\n",
    "        categorical_cols: list of categorical column names\n",
    "        numerical_cols: list of numerical column names\n",
    "        drop_threshold: missing rate threshold for dropping columns\n",
    "        create_indicators: whether to create missing indicator features\n",
    "    \n",
    "    Returns:\n",
    "        Processed train_df, test_df, and imputers dict for inference\n",
    "    \"\"\"\n",
    "    imputers = {}\n",
    "    \n",
    "    # Calculate missing percentages from training data\n",
    "    missing_pct = train_df.isnull().sum() / len(train_df)\n",
    "    \n",
    "    # Identify columns to drop\n",
    "    cols_to_drop = missing_pct[missing_pct > drop_threshold].index.tolist()\n",
    "    cols_to_drop = [c for c in cols_to_drop if c not in ['TransactionID', 'isFraud']]\n",
    "    \n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >{drop_threshold*100}% missing\")\n",
    "    train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Update column lists\n",
    "    categorical_cols = [c for c in categorical_cols if c in train_df.columns]\n",
    "    numerical_cols = [c for c in numerical_cols if c in train_df.columns]\n",
    "    \n",
    "    # Create missing indicators for columns with significant missing values\n",
    "    # This captures the information that data is missing (which can be predictive)\n",
    "    if create_indicators:\n",
    "        indicator_cols = missing_pct[(missing_pct > 0.10) & (missing_pct <= drop_threshold)].index.tolist()\n",
    "        indicator_cols = [c for c in indicator_cols if c in train_df.columns]\n",
    "        \n",
    "        for col in indicator_cols[:20]:  # Limit to top 20 to avoid feature explosion\n",
    "            train_df[f'{col}_missing'] = train_df[col].isnull().astype(np.int8)\n",
    "            test_df[f'{col}_missing'] = test_df[col].isnull().astype(np.int8)\n",
    "        \n",
    "        print(f\"Created {min(20, len(indicator_cols))} missing indicator features\")\n",
    "    \n",
    "    # Impute numerical columns with median\n",
    "    # Median is preferred over mean for fraud data due to outliers\n",
    "    numerical_cols_present = [c for c in numerical_cols if c in train_df.columns]\n",
    "    \n",
    "    for col in numerical_cols_present:\n",
    "        if train_df[col].isnull().sum() > 0:\n",
    "            median_val = train_df[col].median()\n",
    "            imputers[col] = {'strategy': 'median', 'value': median_val}\n",
    "            train_df[col] = train_df[col].fillna(median_val)\n",
    "            test_df[col] = test_df[col].fillna(median_val)\n",
    "    \n",
    "    # Impute categorical columns\n",
    "    categorical_cols_present = [c for c in categorical_cols if c in train_df.columns]\n",
    "    \n",
    "    for col in categorical_cols_present:\n",
    "        if train_df[col].isnull().sum() > 0:\n",
    "            # For string columns, use 'missing' category\n",
    "            if train_df[col].dtype == 'object':\n",
    "                fill_val = 'missing'\n",
    "            else:\n",
    "                # For numeric categorical, use -999 as indicator\n",
    "                fill_val = -999\n",
    "            \n",
    "            imputers[col] = {'strategy': 'constant', 'value': fill_val}\n",
    "            train_df[col] = train_df[col].fillna(fill_val)\n",
    "            test_df[col] = test_df[col].fillna(fill_val)\n",
    "    \n",
    "    print(f\"Imputed {len([c for c in numerical_cols_present if c in imputers])} numerical columns\")\n",
    "    print(f\"Imputed {len([c for c in categorical_cols_present if c in imputers])} categorical columns\")\n",
    "    \n",
    "    return train_df, test_df, imputers, categorical_cols_present, numerical_cols_present\n",
    "\n",
    "# Apply missing value handling\n",
    "train_df, test_df, imputers, CATEGORICAL_COLS, NUMERICAL_COLS = handle_missing_values(\n",
    "    train_df, test_df, CATEGORICAL_COLS, NUMERICAL_COLS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ff8d9",
   "metadata": {},
   "source": [
    "## 4. Temporal Feature Engineering\n",
    "\n",
    "TransactionDT is seconds from a reference datetime. We extract:\n",
    "- Hour of day (fraud patterns vary by hour)\n",
    "- Day of week (weekend vs weekday patterns)\n",
    "- Day of month (beginning/end of month patterns)\n",
    "- Time since start (for trend analysis)\n",
    "\n",
    "**Why this matters**: Fraudsters often operate at unusual hours to avoid detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create temporal features from TransactionDT.\n",
    "    \n",
    "    TransactionDT is timedelta from a given reference datetime.\n",
    "    We extract cyclical time features that capture fraud patterns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with TransactionDT column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with new temporal features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time extractions\n",
    "    df['hour'] = (df['TransactionDT'] // 3600) % 24\n",
    "    df['day'] = df['TransactionDT'] // (24 * 3600)\n",
    "    df['day_of_week'] = df['day'] % 7\n",
    "    df['day_of_month'] = df['day'] % 30\n",
    "    \n",
    "    # Cyclical encoding for hour (captures that 23:00 is close to 00:00)\n",
    "    # This is important because simple hour values don't capture cyclical nature\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    # Cyclical encoding for day of week\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Time-based flags (business hours vs off-hours)\n",
    "    # Fraud is often higher during off-hours when monitoring may be reduced\n",
    "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 5)).astype(np.int8)\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int8)\n",
    "    df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17) & \n",
    "                               (df['day_of_week'] < 5)).astype(np.int8)\n",
    "    \n",
    "    print(f\"Created temporal features: hour, day, day_of_week, day_of_month, \"\n",
    "          f\"hour_sin, hour_cos, dow_sin, dow_cos, is_night, is_weekend, is_business_hours\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_temporal_features(train_df)\n",
    "test_df = create_temporal_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeb225",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding\n",
    "\n",
    "### Encoding Strategy:\n",
    "1. **Label Encoding**: For tree-based models (LightGBM, XGBoost, RF)\n",
    "2. **Frequency Encoding**: Captures commonality of category\n",
    "3. **Target Encoding**: For high-cardinality features (with regularization to prevent leakage)\n",
    "\n",
    "**Note**: We use label encoding primarily since our models are tree-based.\n",
    "Target encoding is applied carefully to avoid target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_features(train_df, test_df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Label encode categorical features for tree-based models.\n",
    "    \n",
    "    Handles unseen categories in test set by assigning -1.\n",
    "    \n",
    "    Args:\n",
    "        train_df, test_df: DataFrames\n",
    "        categorical_cols: list of columns to encode\n",
    "    \n",
    "    Returns:\n",
    "        Encoded DataFrames and encoder dict for inference\n",
    "    \"\"\"\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "            \n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Combine train and test for fitting to handle all categories\n",
    "        combined = pd.concat([train_df[col].astype(str), \n",
    "                              test_df[col].astype(str)], axis=0)\n",
    "        le.fit(combined)\n",
    "        \n",
    "        # Transform\n",
    "        train_df[col] = le.transform(train_df[col].astype(str))\n",
    "        test_df[col] = le.transform(test_df[col].astype(str))\n",
    "        \n",
    "        encoders[col] = le\n",
    "    \n",
    "    print(f\"Label encoded {len(encoders)} categorical columns\")\n",
    "    return train_df, test_df, encoders\n",
    "\n",
    "train_df, test_df, label_encoders = label_encode_features(\n",
    "    train_df, test_df, CATEGORICAL_COLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_encoding(train_df, test_df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Create frequency encoding for categorical features.\n",
    "    \n",
    "    Frequency encoding captures how common a category is.\n",
    "    Rare categories might be more suspicious (fraud attempts with fake info).\n",
    "    \n",
    "    Args:\n",
    "        train_df, test_df: DataFrames\n",
    "        categorical_cols: columns to encode\n",
    "    \n",
    "    Returns:\n",
    "        DataFrames with frequency encoded features\n",
    "    \"\"\"\n",
    "    freq_maps = {}\n",
    "    \n",
    "    # Select high-cardinality columns for frequency encoding\n",
    "    high_card_cols = [col for col in categorical_cols \n",
    "                      if col in train_df.columns and train_df[col].nunique() > 10]\n",
    "    \n",
    "    for col in high_card_cols[:10]:  # Limit to prevent feature explosion\n",
    "        freq_map = train_df[col].value_counts(normalize=True).to_dict()\n",
    "        freq_maps[col] = freq_map\n",
    "        \n",
    "        # Create frequency feature\n",
    "        train_df[f'{col}_freq'] = train_df[col].map(freq_map).fillna(0).astype(np.float32)\n",
    "        test_df[f'{col}_freq'] = test_df[col].map(freq_map).fillna(0).astype(np.float32)\n",
    "    \n",
    "    print(f\"Created {len(freq_maps)} frequency encoded features\")\n",
    "    return train_df, test_df, freq_maps\n",
    "\n",
    "train_df, test_df, freq_encoders = create_frequency_encoding(\n",
    "    train_df, test_df, CATEGORICAL_COLS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4ba70",
   "metadata": {},
   "source": [
    "## 6. Interaction Features\n",
    "\n",
    "Creating interaction features to capture fraud patterns:\n",
    "- Card + Address combinations (legitimate users have consistent patterns)\n",
    "- Email domain patterns (free email vs corporate)\n",
    "- Device + Card combinations\n",
    "\n",
    "**Why this matters**: Fraudsters often show inconsistent combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features capturing suspicious patterns.\n",
    "    \n",
    "    Fraud detection benefits from features that capture:\n",
    "    - Unusual combinations (card from one region, address from another)\n",
    "    - Email domain characteristics\n",
    "    - Card usage patterns\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with interaction features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Card + Address interaction\n",
    "    # Legitimate users typically have consistent card-address combinations\n",
    "    if 'card1' in df.columns and 'addr1' in df.columns:\n",
    "        df['card1_addr1'] = df['card1'].astype(str) + '_' + df['addr1'].astype(str)\n",
    "        # Encode the interaction\n",
    "        df['card1_addr1'] = LabelEncoder().fit_transform(df['card1_addr1'])\n",
    "    \n",
    "    # Card type interactions\n",
    "    if 'card4' in df.columns and 'card6' in df.columns:\n",
    "        df['card4_card6'] = df['card4'].astype(str) + '_' + df['card6'].astype(str)\n",
    "        df['card4_card6'] = LabelEncoder().fit_transform(df['card4_card6'])\n",
    "    \n",
    "    # Transaction amount features\n",
    "    if 'TransactionAmt' in df.columns:\n",
    "        # Log transform to handle skewness\n",
    "        df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])\n",
    "        \n",
    "        # Decimal part - unusual amounts might indicate fraud\n",
    "        df['TransactionAmt_decimal'] = (df['TransactionAmt'] - \n",
    "                                        np.floor(df['TransactionAmt'])).astype(np.float32)\n",
    "        \n",
    "        # Is round amount (whole dollar)\n",
    "        df['is_round_amount'] = (df['TransactionAmt_decimal'] < 0.01).astype(np.int8)\n",
    "    \n",
    "    # Email domain features\n",
    "    # Note: P_emaildomain is purchaser, R_emaildomain is recipient\n",
    "    # Mismatch might indicate fraud\n",
    "    if 'P_emaildomain' in df.columns and 'R_emaildomain' in df.columns:\n",
    "        df['email_match'] = (df['P_emaildomain'] == df['R_emaildomain']).astype(np.int8)\n",
    "    \n",
    "    # Browser/Device features from identity columns\n",
    "    if 'DeviceType' in df.columns and 'DeviceInfo' in df.columns:\n",
    "        df['device_type_info'] = df['DeviceType'].astype(str) + '_' + df['DeviceInfo'].astype(str)\n",
    "        df['device_type_info'] = LabelEncoder().fit_transform(df['device_type_info'])\n",
    "    \n",
    "    print(\"Created interaction features: card1_addr1, card4_card6, TransactionAmt_log, \"\n",
    "          \"TransactionAmt_decimal, is_round_amount, email_match, device_type_info\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_interaction_features(train_df)\n",
    "test_df = create_interaction_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abb2de",
   "metadata": {},
   "source": [
    "## 7. Aggregation Features\n",
    "\n",
    "Creating aggregation features to capture patterns:\n",
    "- Transaction counts per card\n",
    "- Average transaction amount per card\n",
    "- Time since last transaction\n",
    "\n",
    "**Why this matters**: Fraudsters often make multiple rapid transactions before being caught."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregation_features(train_df, test_df, group_cols=['card1', 'card2', 'addr1']):\n",
    "    \"\"\"\n",
    "    Create aggregation features based on grouping columns.\n",
    "    \n",
    "    Aggregations capture user behavior patterns:\n",
    "    - How many transactions from this card?\n",
    "    - What's the average amount for this card?\n",
    "    - How does this transaction compare to the card's typical behavior?\n",
    "    \n",
    "    Args:\n",
    "        train_df, test_df: DataFrames\n",
    "        group_cols: columns to group by for aggregations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrames with aggregation features\n",
    "    \"\"\"\n",
    "    agg_features = {}\n",
    "    \n",
    "    for col in group_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Calculate aggregations on training data\n",
    "        agg = train_df.groupby(col).agg({\n",
    "            'TransactionAmt': ['count', 'mean', 'std', 'min', 'max']\n",
    "        })\n",
    "        agg.columns = [f'{col}_TransactionAmt_{stat}' for stat in ['count', 'mean', 'std', 'min', 'max']]\n",
    "        agg = agg.reset_index()\n",
    "        \n",
    "        # Store for later use\n",
    "        agg_features[col] = agg.set_index(col).to_dict('index')\n",
    "        \n",
    "        # Merge aggregations\n",
    "        train_df = train_df.merge(agg, on=col, how='left')\n",
    "        test_df = test_df.merge(agg, on=col, how='left')\n",
    "        \n",
    "        # Fill missing (new cards in test) with global statistics\n",
    "        for stat_col in agg.columns:\n",
    "            if stat_col != col:\n",
    "                global_val = train_df[stat_col].median()\n",
    "                train_df[stat_col] = train_df[stat_col].fillna(global_val).astype(np.float32)\n",
    "                test_df[stat_col] = test_df[stat_col].fillna(global_val).astype(np.float32)\n",
    "        \n",
    "        # Create ratio features: how does this transaction compare to typical?\n",
    "        mean_col = f'{col}_TransactionAmt_mean'\n",
    "        if mean_col in train_df.columns:\n",
    "            train_df[f'{col}_amt_ratio'] = (train_df['TransactionAmt'] / \n",
    "                                            (train_df[mean_col] + 1)).astype(np.float32)\n",
    "            test_df[f'{col}_amt_ratio'] = (test_df['TransactionAmt'] / \n",
    "                                           (test_df[mean_col] + 1)).astype(np.float32)\n",
    "    \n",
    "    print(f\"Created aggregation features for: {[c for c in group_cols if c in train_df.columns]}\")\n",
    "    return train_df, test_df, agg_features\n",
    "\n",
    "train_df, test_df, agg_features = create_aggregation_features(\n",
    "    train_df, test_df, group_cols=['card1', 'card2', 'addr1']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32629da8",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n",
    "\n",
    "Remove features that:\n",
    "- Have zero variance (no predictive value)\n",
    "- Are highly correlated with each other (redundant)\n",
    "- Have too many unique values (potential overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37116ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(train_df, test_df, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Remove features with variance below threshold.\n",
    "    \n",
    "    Constant or near-constant features provide no information gain.\n",
    "    \n",
    "    Args:\n",
    "        train_df, test_df: DataFrames\n",
    "        threshold: minimum variance required\n",
    "    \n",
    "    Returns:\n",
    "        DataFrames with constant features removed\n",
    "    \"\"\"\n",
    "    # Get numerical columns\n",
    "    numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numerical_cols = [c for c in numerical_cols if c not in ['TransactionID', 'isFraud']]\n",
    "    \n",
    "    # Calculate variance\n",
    "    variances = train_df[numerical_cols].var()\n",
    "    low_var_cols = variances[variances < threshold].index.tolist()\n",
    "    \n",
    "    if low_var_cols:\n",
    "        train_df = train_df.drop(columns=low_var_cols)\n",
    "        test_df = test_df.drop(columns=low_var_cols, errors='ignore')\n",
    "        print(f\"Removed {len(low_var_cols)} low variance features\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = remove_constant_features(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature list\n",
    "exclude_cols = ['TransactionID', 'isFraud', 'TransactionDT']\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"\\nFinal number of features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea32bc",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data\n",
    "\n",
    "Save the processed datasets and feature engineering artifacts for:\n",
    "1. Model training\n",
    "2. Inference pipeline (encoders, imputers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "# Save as parquet for efficiency (better than CSV for large datasets)\n",
    "train_df.to_parquet(PROCESSED_PATH / 'train_processed.parquet', index=False)\n",
    "test_df.to_parquet(PROCESSED_PATH / 'test_processed.parquet', index=False)\n",
    "\n",
    "print(f\"Saved training data: {PROCESSED_PATH / 'train_processed.parquet'}\")\n",
    "print(f\"Saved test data: {PROCESSED_PATH / 'test_processed.parquet'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6092cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature engineering artifacts for inference\n",
    "feature_artifacts = {\n",
    "    'imputers': imputers,\n",
    "    'label_encoders': label_encoders,\n",
    "    'freq_encoders': freq_encoders,\n",
    "    'agg_features': agg_features,\n",
    "    'feature_cols': feature_cols,\n",
    "    'categorical_cols': CATEGORICAL_COLS,\n",
    "    'numerical_cols': NUMERICAL_COLS\n",
    "}\n",
    "\n",
    "with open(FEATURES_PATH / 'feature_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_artifacts, f)\n",
    "\n",
    "print(f\"Saved feature artifacts: {FEATURES_PATH / 'feature_artifacts.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadedc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature list for documentation\n",
    "feature_info = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'dtype': [str(train_df[c].dtype) for c in feature_cols],\n",
    "    'nunique': [train_df[c].nunique() for c in feature_cols],\n",
    "    'missing_pct': [train_df[c].isnull().sum() / len(train_df) * 100 for c in feature_cols]\n",
    "})\n",
    "\n",
    "feature_info.to_csv(OUTPUT_PATH / 'metrics' / 'feature_info.csv', index=False)\n",
    "print(f\"Saved feature info: {OUTPUT_PATH / 'metrics' / 'feature_info.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal features: ~434\")\n",
    "print(f\"Final features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature types created:\")\n",
    "print(\"  - Temporal: hour, day, day_of_week, cyclical encodings, time flags\")\n",
    "print(\"  - Interactions: card-address, card-type, amount features\")\n",
    "print(\"  - Aggregations: transaction stats per card/address\")\n",
    "print(\"  - Frequency encodings: category frequencies\")\n",
    "print(\"  - Missing indicators: capturing missing patterns\")\n",
    "print(f\"\\nData saved to: {PROCESSED_PATH}\")\n",
    "print(f\"Artifacts saved to: {FEATURES_PATH}\")\n",
    "print(\"\\nNext steps: Proceed to 03_modeling.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
