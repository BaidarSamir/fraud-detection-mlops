{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bddf17f",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection - Exploratory Data Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive exploratory data analysis on the IEEE-CIS Fraud Detection dataset.\n",
    "The goal is to understand the data structure, identify patterns, and inform feature engineering decisions.\n",
    "\n",
    "## Dataset Description\n",
    "- **Transaction Data**: Contains transaction details including amount, card info, addresses, etc.\n",
    "- **Identity Data**: Contains identity information linked to transactions via TransactionID\n",
    "- **Target Variable**: `isFraud` - binary indicator of fraudulent transactions\n",
    "\n",
    "## Key Questions to Answer\n",
    "1. What is the class distribution (expected severe imbalance)?\n",
    "2. What are the missing value patterns?\n",
    "3. What temporal patterns exist in fraud?\n",
    "4. Which features show strongest correlation with fraud?\n",
    "5. What feature engineering opportunities exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ea755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Define paths - using relative paths from notebooks directory\n",
    "BASE_PATH = Path('..').resolve()\n",
    "DATA_PATH = BASE_PATH / 'Data' / 'raw'\n",
    "OUTPUT_PATH = BASE_PATH / 'outputs' / 'visuals'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path: {DATA_PATH}\")\n",
    "print(f\"Output Path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f207cd",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection\n",
    "\n",
    "We load both transaction and identity datasets separately first to understand their individual structures,\n",
    "then merge them for comprehensive analysis. The merge is done on `TransactionID` using a left join\n",
    "because not all transactions have associated identity information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09763c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data\n",
    "# Note: This is a large dataset, so we monitor memory usage\n",
    "print(\"Loading transaction data...\")\n",
    "train_transaction = pd.read_csv(DATA_PATH / 'train_transaction.csv')\n",
    "print(f\"Transaction shape: {train_transaction.shape}\")\n",
    "print(f\"Memory usage: {train_transaction.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nLoading identity data...\")\n",
    "train_identity = pd.read_csv(DATA_PATH / 'train_identity.csv')\n",
    "print(f\"Identity shape: {train_identity.shape}\")\n",
    "print(f\"Memory usage: {train_identity.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transaction and identity data\n",
    "# Using left join because not all transactions have identity information\n",
    "# This is important: missing identity data itself might be a signal for fraud\n",
    "print(\"Merging datasets...\")\n",
    "train_df = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "print(f\"Merged dataset shape: {train_df.shape}\")\n",
    "print(f\"Transactions with identity info: {train_identity.shape[0]:,} ({train_identity.shape[0]/train_transaction.shape[0]*100:.1f}%)\")\n",
    "print(f\"Transactions without identity info: {train_transaction.shape[0] - train_identity.shape[0]:,}\")\n",
    "\n",
    "# Free memory by deleting original dataframes\n",
    "del train_transaction, train_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data inspection\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Rows: {train_df.shape[0]:,}\")\n",
    "print(f\"Total Columns: {train_df.shape[1]}\")\n",
    "print(f\"\\nColumn Data Types:\")\n",
    "print(train_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7096de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of key columns\n",
    "key_columns = ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', \n",
    "               'ProductCD', 'card1', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain']\n",
    "train_df[key_columns].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9987970",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis\n",
    "\n",
    "Understanding the class distribution is critical for fraud detection.\n",
    "We expect severe class imbalance (typically 1-5% fraud rate) which has significant implications:\n",
    "- Standard accuracy metrics will be misleading\n",
    "- We need specialized techniques (SMOTE, class weights, undersampling)\n",
    "- Evaluation should focus on precision-recall rather than accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "fraud_counts = train_df['isFraud'].value_counts()\n",
    "fraud_pct = train_df['isFraud'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nLegitimate Transactions (0): {fraud_counts[0]:,} ({fraud_pct[0]:.2f}%)\")\n",
    "print(f\"Fraudulent Transactions (1): {fraud_counts[1]:,} ({fraud_pct[1]:.2f}%)\")\n",
    "print(f\"\\nClass Imbalance Ratio: 1:{fraud_counts[0]//fraud_counts[1]}\")\n",
    "print(\"\\nIMPLICATIONS:\")\n",
    "print(\"- Severe class imbalance requires specialized handling\")\n",
    "print(\"- Accuracy metric would be misleading (>96% by predicting all 0s)\")\n",
    "print(\"- Will use precision-recall AUC as primary metric\")\n",
    "print(\"- Consider SMOTE or class weights during modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180486bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Target Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[0].bar(['Legitimate', 'Fraud'], fraud_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Transaction Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_yscale('log')  # Log scale due to imbalance\n",
    "for bar, count in zip(bars, fraud_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                 f'{count:,}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(fraud_counts.values, labels=['Legitimate', 'Fraud'], autopct='%1.2f%%',\n",
    "            colors=colors, explode=[0, 0.1], startangle=90)\n",
    "axes[1].set_title('Fraud Rate Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '01_target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '01_target_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1202a",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis\n",
    "\n",
    "Missing values are common in fraud detection datasets and can carry information.\n",
    "Key considerations:\n",
    "- High missing rates in identity features (expected - not all transactions have identity info)\n",
    "- V-columns (Vesta engineered features) have structured missing patterns\n",
    "- Missing data patterns themselves can be predictive of fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_df = pd.DataFrame({\n",
    "    'column': train_df.columns,\n",
    "    'missing_count': train_df.isnull().sum().values,\n",
    "    'missing_pct': (train_df.isnull().sum() / len(train_df) * 100).values\n",
    "}).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns with no missing values: {(missing_df['missing_pct'] == 0).sum()}\")\n",
    "print(f\"Columns with <10% missing: {((missing_df['missing_pct'] > 0) & (missing_df['missing_pct'] < 10)).sum()}\")\n",
    "print(f\"Columns with 10-50% missing: {((missing_df['missing_pct'] >= 10) & (missing_df['missing_pct'] < 50)).sum()}\")\n",
    "print(f\"Columns with 50-90% missing: {((missing_df['missing_pct'] >= 50) & (missing_df['missing_pct'] < 90)).sum()}\")\n",
    "print(f\"Columns with >90% missing: {(missing_df['missing_pct'] >= 90).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d614bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 columns with highest missing values\n",
    "print(\"\\nTop 30 Columns with Highest Missing Values:\")\n",
    "print(missing_df.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Missing Value Heatmap\n",
    "# Select columns with significant missing values for visualization\n",
    "cols_with_missing = missing_df[missing_df['missing_pct'] > 0]['column'].tolist()[:50]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Sample data for visualization (full dataset too large for heatmap)\n",
    "sample_idx = np.random.choice(train_df.index, size=min(5000, len(train_df)), replace=False)\n",
    "missing_matrix = train_df.loc[sample_idx, cols_with_missing].isnull().astype(int)\n",
    "\n",
    "sns.heatmap(missing_matrix.T, cbar=True, cmap='YlOrRd', \n",
    "            yticklabels=True, xticklabels=False)\n",
    "ax.set_title('Missing Value Patterns (Top 50 Columns with Missing Data)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Transactions (sampled)', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '02_missing_value_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '02_missing_value_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57213eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Missing Values Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Top 30 columns with missing values\n",
    "top_missing = missing_df[missing_df['missing_pct'] > 0].head(30)\n",
    "\n",
    "bars = ax.barh(top_missing['column'], top_missing['missing_pct'], color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
    "ax.set_title('Top 30 Features by Missing Value Percentage', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=50, color='red', linestyle='--', label='50% threshold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '03_missing_values_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '03_missing_values_bar.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7df788",
   "metadata": {},
   "source": [
    "## 4. Feature Type Identification\n",
    "\n",
    "Categorizing features is essential for appropriate preprocessing:\n",
    "- **Numerical**: Continuous values (amounts, counts, V-features)\n",
    "- **Categorical**: Discrete categories (card types, product codes, device info)\n",
    "- **Temporal**: Time-related (TransactionDT)\n",
    "- **ID-like**: High cardinality identifiers (card1, addr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "def categorize_features(df):\n",
    "    \"\"\"Categorize features into numerical, categorical, and temporal types.\"\"\"\n",
    "    \n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    id_cols = ['TransactionID']  # Explicitly mark ID columns\n",
    "    target_col = ['isFraud']\n",
    "    temporal_cols = ['TransactionDT']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in id_cols + target_col + temporal_cols:\n",
    "            continue\n",
    "            \n",
    "        # Check if column is object type (usually categorical)\n",
    "        if df[col].dtype == 'object':\n",
    "            categorical_cols.append(col)\n",
    "        # Check if numerical with few unique values (likely categorical)\n",
    "        elif df[col].nunique() < 20 and df[col].dtype in ['int64', 'float64']:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            numerical_cols.append(col)\n",
    "    \n",
    "    return {\n",
    "        'numerical': numerical_cols,\n",
    "        'categorical': categorical_cols,\n",
    "        'temporal': temporal_cols,\n",
    "        'id': id_cols,\n",
    "        'target': target_col\n",
    "    }\n",
    "\n",
    "feature_types = categorize_features(train_df)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNumerical features: {len(feature_types['numerical'])}\")\n",
    "print(f\"Categorical features: {len(feature_types['categorical'])}\")\n",
    "print(f\"Temporal features: {len(feature_types['temporal'])}\")\n",
    "print(f\"ID features: {len(feature_types['id'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed feature groups based on naming conventions\n",
    "feature_groups = {\n",
    "    'Card Features': [c for c in train_df.columns if c.startswith('card')],\n",
    "    'Address Features': [c for c in train_df.columns if c.startswith('addr')],\n",
    "    'Email Features': [c for c in train_df.columns if 'email' in c.lower()],\n",
    "    'C Features (Counting)': [c for c in train_df.columns if c.startswith('C') and c[1:].isdigit()],\n",
    "    'D Features (Timedelta)': [c for c in train_df.columns if c.startswith('D') and c[1:].isdigit()],\n",
    "    'M Features (Match)': [c for c in train_df.columns if c.startswith('M') and c[1:].isdigit()],\n",
    "    'V Features (Vesta)': [c for c in train_df.columns if c.startswith('V')],\n",
    "    'Identity Features': [c for c in train_df.columns if c.startswith('id_')],\n",
    "    'Device Features': [c for c in train_df.columns if 'Device' in c]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature Groups:\")\n",
    "for group, cols in feature_groups.items():\n",
    "    print(f\"  {group}: {len(cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e94811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features - cardinality analysis\n",
    "print(\"\\nCategorical Feature Cardinality (Top 20):\")\n",
    "cardinality = {col: train_df[col].nunique() for col in feature_types['categorical']}\n",
    "cardinality_sorted = sorted(cardinality.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "for col, nunique in cardinality_sorted:\n",
    "    print(f\"  {col}: {nunique:,} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636eab5",
   "metadata": {},
   "source": [
    "## 5. Transaction Amount Analysis\n",
    "\n",
    "Transaction amount is often a key indicator for fraud detection.\n",
    "We analyze:\n",
    "- Distribution of amounts for fraud vs legitimate\n",
    "- Statistical differences between classes\n",
    "- Outlier patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Amount Statistics by Fraud Status\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSACTION AMOUNT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "amount_stats = train_df.groupby('isFraud')['TransactionAmt'].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
    "amount_stats.index = ['Legitimate', 'Fraud']\n",
    "print(\"\\nTransaction Amount Statistics:\")\n",
    "print(amount_stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faadf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Transaction Amount Distribution by Fraud Status\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Log-transformed distribution\n",
    "for fraud_val, color, label in [(0, '#2ecc71', 'Legitimate'), (1, '#e74c3c', 'Fraud')]:\n",
    "    subset = train_df[train_df['isFraud'] == fraud_val]['TransactionAmt']\n",
    "    axes[0].hist(np.log1p(subset), bins=100, alpha=0.6, color=color, label=label, density=True)\n",
    "\n",
    "axes[0].set_xlabel('Log(Transaction Amount + 1)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Transaction Amount Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "train_df.boxplot(column='TransactionAmt', by='isFraud', ax=axes[1])\n",
    "axes[1].set_xlabel('Fraud Status', fontsize=12)\n",
    "axes[1].set_ylabel('Transaction Amount', fontsize=12)\n",
    "axes[1].set_title('Transaction Amount by Fraud Status', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(['Legitimate', 'Fraud'])\n",
    "axes[1].set_yscale('log')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '04_transaction_amount_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '04_transaction_amount_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a19d1",
   "metadata": {},
   "source": [
    "## 6. Temporal Pattern Analysis\n",
    "\n",
    "TransactionDT represents seconds from a reference time. Fraud often shows temporal patterns:\n",
    "- Time of day effects (late night transactions)\n",
    "- Day of week patterns\n",
    "- Burst patterns (multiple rapid transactions)\n",
    "\n",
    "**Note**: We use time-based train/test split for modeling to simulate production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ae03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features for analysis\n",
    "# TransactionDT is in seconds, we convert to days and extract patterns\n",
    "train_df['TransactionDay'] = train_df['TransactionDT'] // (24 * 60 * 60)\n",
    "train_df['TransactionHour'] = (train_df['TransactionDT'] // 3600) % 24\n",
    "train_df['TransactionDayOfWeek'] = (train_df['TransactionDT'] // (24 * 60 * 60)) % 7\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTransaction time span: {train_df['TransactionDay'].min()} to {train_df['TransactionDay'].max()} days\")\n",
    "print(f\"Total days of data: {train_df['TransactionDay'].nunique()} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228be13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Fraud Rate by Hour of Day\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Hourly fraud rate\n",
    "hourly_fraud = train_df.groupby('TransactionHour')['isFraud'].mean() * 100\n",
    "axes[0].bar(hourly_fraud.index, hourly_fraud.values, color='steelblue', edgecolor='black')\n",
    "axes[0].axhline(y=train_df['isFraud'].mean() * 100, color='red', linestyle='--', label='Overall Fraud Rate')\n",
    "axes[0].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[0].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[0].set_title('Fraud Rate by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(range(24))\n",
    "\n",
    "# Transaction volume by hour\n",
    "hourly_volume = train_df.groupby('TransactionHour').size()\n",
    "axes[1].bar(hourly_volume.index, hourly_volume.values, color='lightblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[1].set_ylabel('Transaction Count', fontsize=12)\n",
    "axes[1].set_title('Transaction Volume by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(24))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '05_hourly_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '05_hourly_patterns.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Fraud Rate Over Time (Daily)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Daily fraud rate trend\n",
    "daily_stats = train_df.groupby('TransactionDay').agg({\n",
    "    'isFraud': ['sum', 'count', 'mean']\n",
    "}).reset_index()\n",
    "daily_stats.columns = ['Day', 'FraudCount', 'TotalCount', 'FraudRate']\n",
    "\n",
    "axes[0].plot(daily_stats['Day'], daily_stats['FraudRate'] * 100, color='red', alpha=0.7)\n",
    "axes[0].fill_between(daily_stats['Day'], daily_stats['FraudRate'] * 100, alpha=0.3, color='red')\n",
    "axes[0].axhline(y=train_df['isFraud'].mean() * 100, color='black', linestyle='--', label='Overall Mean')\n",
    "axes[0].set_xlabel('Day', fontsize=12)\n",
    "axes[0].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[0].set_title('Daily Fraud Rate Trend', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Transaction volume over time\n",
    "axes[1].bar(daily_stats['Day'], daily_stats['TotalCount'], color='steelblue', alpha=0.7)\n",
    "axes[1].set_xlabel('Day', fontsize=12)\n",
    "axes[1].set_ylabel('Transaction Count', fontsize=12)\n",
    "axes[1].set_title('Daily Transaction Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '06_daily_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '06_daily_trends.png'}\")\n",
    "\n",
    "print(\"\\nOBSERVATIONS:\")\n",
    "print(\"- Fraud rate varies over time, suggesting concept drift considerations\")\n",
    "print(\"- This justifies using time-based split for training/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39f167",
   "metadata": {},
   "source": [
    "## 7. Categorical Feature Analysis\n",
    "\n",
    "Analyzing key categorical features to understand fraud patterns:\n",
    "- ProductCD: Product type\n",
    "- Card types (card4, card6)\n",
    "- Email domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e414097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProductCD Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"PRODUCT CODE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "product_analysis = train_df.groupby('ProductCD').agg({\n",
    "    'isFraud': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "product_analysis.columns = ['FraudCount', 'TotalCount', 'FraudRate']\n",
    "product_analysis['FraudRate'] = product_analysis['FraudRate'] * 100\n",
    "product_analysis = product_analysis.sort_values('FraudRate', ascending=False)\n",
    "print(product_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980239fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: Fraud Rate by Key Categorical Features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ProductCD\n",
    "product_fraud = train_df.groupby('ProductCD')['isFraud'].mean() * 100\n",
    "product_fraud.sort_values(ascending=True).plot(kind='barh', ax=axes[0, 0], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[0, 0].set_title('Fraud Rate by Product Code', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(x=train_df['isFraud'].mean() * 100, color='red', linestyle='--', label='Overall')\n",
    "\n",
    "# Card4 (Visa, Mastercard, etc.)\n",
    "card4_fraud = train_df.groupby('card4')['isFraud'].mean() * 100\n",
    "card4_fraud.sort_values(ascending=True).plot(kind='barh', ax=axes[0, 1], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Fraud Rate by Card Network (card4)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(x=train_df['isFraud'].mean() * 100, color='red', linestyle='--', label='Overall')\n",
    "\n",
    "# Card6 (Credit, Debit)\n",
    "card6_fraud = train_df.groupby('card6')['isFraud'].mean() * 100\n",
    "card6_fraud.sort_values(ascending=True).plot(kind='barh', ax=axes[1, 0], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Fraud Rate by Card Type (card6)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axvline(x=train_df['isFraud'].mean() * 100, color='red', linestyle='--', label='Overall')\n",
    "\n",
    "# Top email domains (fraud rate)\n",
    "email_fraud = train_df.groupby('P_emaildomain').agg({\n",
    "    'isFraud': ['sum', 'count', 'mean']\n",
    "})\n",
    "email_fraud.columns = ['fraud_count', 'total', 'fraud_rate']\n",
    "email_fraud = email_fraud[email_fraud['total'] > 1000]  # Filter for significance\n",
    "email_fraud = email_fraud.sort_values('fraud_rate', ascending=True).tail(15)\n",
    "email_fraud['fraud_rate'].mul(100).plot(kind='barh', ax=axes[1, 1], color='purple', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Fraud Rate by Email Domain (Top 15, min 1000 txns)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axvline(x=train_df['isFraud'].mean() * 100, color='red', linestyle='--', label='Overall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '07_categorical_fraud_rates.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '07_categorical_fraud_rates.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c9425",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis\n",
    "\n",
    "Analyzing correlations between numerical features and the target variable.\n",
    "This helps identify the most predictive features and potential multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a63191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target for numerical features\n",
    "numerical_features = feature_types['numerical']\n",
    "\n",
    "# Calculate correlations (using only non-null values)\n",
    "correlations = {}\n",
    "for col in numerical_features:\n",
    "    if train_df[col].notna().sum() > 1000:  # Only calculate for columns with enough data\n",
    "        corr = train_df[[col, 'isFraud']].dropna().corr().iloc[0, 1]\n",
    "        correlations[col] = corr\n",
    "\n",
    "corr_df = pd.DataFrame({\n",
    "    'feature': list(correlations.keys()),\n",
    "    'correlation': list(correlations.values())\n",
    "})\n",
    "corr_df['abs_correlation'] = corr_df['correlation'].abs()\n",
    "corr_df = corr_df.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP 30 FEATURES BY CORRELATION WITH FRAUD\")\n",
    "print(\"=\" * 60)\n",
    "print(corr_df.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff562a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 8: Top Feature Correlations with Fraud\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "top_corr = corr_df.head(25)\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in top_corr['correlation']]\n",
    "ax.barh(top_corr['feature'], top_corr['correlation'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Correlation with Fraud', fontsize=12)\n",
    "ax.set_title('Top 25 Features by Correlation with Fraud', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '08_feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '08_feature_correlations.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 9: Correlation Matrix of Top Features\n",
    "top_features = corr_df.head(15)['feature'].tolist() + ['isFraud']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "corr_matrix = train_df[top_features].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, square=True, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Correlation Matrix of Top 15 Features', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '09_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '09_correlation_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088281b5",
   "metadata": {},
   "source": [
    "## 9. Identity Features Analysis\n",
    "\n",
    "Identity features provide device and browser information.\n",
    "These are particularly valuable for fraud detection as fraudsters often:\n",
    "- Use multiple devices\n",
    "- Have unusual browser configurations\n",
    "- Show different patterns than legitimate users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aba208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity feature analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"IDENTITY FEATURES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check fraud rate for transactions with vs without identity info\n",
    "has_identity = train_df['id_01'].notna()\n",
    "fraud_with_identity = train_df[has_identity]['isFraud'].mean() * 100\n",
    "fraud_without_identity = train_df[~has_identity]['isFraud'].mean() * 100\n",
    "\n",
    "print(f\"\\nFraud rate WITH identity info: {fraud_with_identity:.2f}%\")\n",
    "print(f\"Fraud rate WITHOUT identity info: {fraud_without_identity:.2f}%\")\n",
    "print(\"\\nHaving identity information available is itself predictive of fraud!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228af35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Type Analysis\n",
    "if 'DeviceType' in train_df.columns:\n",
    "    device_fraud = train_df.groupby('DeviceType')['isFraud'].agg(['count', 'sum', 'mean'])\n",
    "    device_fraud.columns = ['Count', 'FraudCount', 'FraudRate']\n",
    "    device_fraud['FraudRate'] = device_fraud['FraudRate'] * 100\n",
    "    print(\"\\nFraud Rate by Device Type:\")\n",
    "    print(device_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 10: Device and Browser Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Device Type\n",
    "if 'DeviceType' in train_df.columns:\n",
    "    device_counts = train_df.groupby(['DeviceType', 'isFraud']).size().unstack(fill_value=0)\n",
    "    device_pct = device_counts.div(device_counts.sum(axis=1), axis=0) * 100\n",
    "    device_pct.plot(kind='bar', stacked=True, ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "    axes[0].set_xlabel('Device Type', fontsize=12)\n",
    "    axes[0].set_ylabel('Percentage', fontsize=12)\n",
    "    axes[0].set_title('Fraud Distribution by Device Type', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(['Legitimate', 'Fraud'], loc='upper right')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Identity presence impact\n",
    "identity_impact = pd.DataFrame({\n",
    "    'Category': ['With Identity', 'Without Identity'],\n",
    "    'FraudRate': [fraud_with_identity, fraud_without_identity]\n",
    "})\n",
    "bars = axes[1].bar(identity_impact['Category'], identity_impact['FraudRate'], \n",
    "                   color=['#3498db', '#95a5a6'], edgecolor='black')\n",
    "axes[1].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[1].set_title('Fraud Rate by Identity Information Availability', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, identity_impact['FraudRate']):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                 f'{val:.2f}%', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / '10_identity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_PATH / '10_identity_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edb0a2",
   "metadata": {},
   "source": [
    "## 10. Key Findings Summary\n",
    "\n",
    "### Data Characteristics\n",
    "1. **Severe Class Imbalance**: ~3.5% fraud rate requires specialized handling\n",
    "2. **High Dimensionality**: 400+ features, many with high missing rates\n",
    "3. **Mixed Feature Types**: Numerical (V-features), categorical (cards, emails), temporal\n",
    "\n",
    "### Feature Engineering Opportunities\n",
    "1. **Temporal Features**: Hour, day, day-of-week patterns show fraud variation\n",
    "2. **Identity Presence**: Missing identity data correlates with lower fraud\n",
    "3. **Aggregations**: Transaction counts by card, email domain\n",
    "4. **Interaction Features**: Card + address combinations\n",
    "\n",
    "### Modeling Considerations\n",
    "1. **Time-based Split**: Fraud patterns change over time (concept drift)\n",
    "2. **Class Imbalance**: Use SMOTE, class weights, or PR-AUC metric\n",
    "3. **Feature Selection**: Many V-features highly correlated\n",
    "4. **Missing Values**: Strategic imputation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save EDA summary statistics\n",
    "summary_stats = {\n",
    "    'total_transactions': len(train_df),\n",
    "    'fraud_count': train_df['isFraud'].sum(),\n",
    "    'fraud_rate': train_df['isFraud'].mean(),\n",
    "    'total_features': train_df.shape[1],\n",
    "    'numerical_features': len(feature_types['numerical']),\n",
    "    'categorical_features': len(feature_types['categorical']),\n",
    "    'features_with_missing': (train_df.isnull().sum() > 0).sum(),\n",
    "    'avg_transaction_amount': train_df['TransactionAmt'].mean(),\n",
    "    'median_transaction_amount': train_df['TransactionAmt'].median()\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_stats])\n",
    "summary_df.to_csv(BASE_PATH / 'outputs' / 'metrics' / 'eda_summary.csv', index=False)\n",
    "\n",
    "# Save feature types for later use\n",
    "import json\n",
    "with open(BASE_PATH / 'outputs' / 'metrics' / 'feature_types.json', 'w') as f:\n",
    "    json.dump(feature_types, f, indent=2)\n",
    "\n",
    "print(\"EDA Summary saved to outputs/metrics/eda_summary.csv\")\n",
    "print(\"Feature types saved to outputs/metrics/feature_types.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary columns\n",
    "temp_cols = ['TransactionDay', 'TransactionHour', 'TransactionDayOfWeek']\n",
    "train_df.drop(columns=temp_cols, inplace=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EDA COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVisualization files saved to: {OUTPUT_PATH}\")\n",
    "print(\"\\nNext steps: Proceed to 02_feature_engineering.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
