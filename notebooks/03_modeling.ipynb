{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9d5456",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection - Model Training with MLflow\n",
    "\n",
    "## Overview\n",
    "This notebook trains and evaluates multiple models for fraud detection with MLflow experiment tracking.\n",
    "We implement production-grade practices including time-based splits, class imbalance handling,\n",
    "and comprehensive metric logging.\n",
    "\n",
    "## Modeling Strategy\n",
    "\n",
    "### Why Time-Based Split (Not Random)\n",
    "In production, models are trained on historical data and predict future transactions.\n",
    "Random splits would leak future information and give overly optimistic results.\n",
    "Time-based splits simulate real deployment conditions.\n",
    "\n",
    "### Handling Class Imbalance\n",
    "With ~3.5% fraud rate, we use:\n",
    "- **Class weights**: Scale loss function to penalize minority class errors more\n",
    "- **SMOTE**: Considered but avoided due to potential for creating unrealistic samples\n",
    "- **Threshold tuning**: Adjust classification threshold for desired precision-recall trade-off\n",
    "\n",
    "### Models Compared\n",
    "1. **LightGBM**: Fast, handles categorical features natively, good for large datasets\n",
    "2. **XGBoost**: Robust, excellent regularization, widely used in production\n",
    "3. **Random Forest**: Baseline ensemble, interpretable feature importance\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Primary**: Precision-Recall AUC (robust to class imbalance)\n",
    "- **Secondary**: F1-score, ROC-AUC, Recall at high precision\n",
    "- **Business Context**: False positives cost customer friction; false negatives cost fraud losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d87373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, average_precision_score,\n",
    "    f1_score, precision_score, recall_score, confusion_matrix,\n",
    "    classification_report, roc_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.lightgbm\n",
    "import mlflow.xgboost\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Define paths\n",
    "BASE_PATH = Path('..').resolve()\n",
    "PROCESSED_PATH = BASE_PATH / 'Data' / 'processed'\n",
    "FEATURES_PATH = BASE_PATH / 'Data' / 'features'\n",
    "OUTPUT_PATH = BASE_PATH / 'outputs'\n",
    "MODELS_PATH = OUTPUT_PATH / 'models'\n",
    "MLRUNS_PATH = BASE_PATH / 'mlruns'\n",
    "\n",
    "# Create directories\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(BASE_PATH / 'src'))\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"MLflow Tracking URI: {MLRUNS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251323c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(f\"file:///{MLRUNS_PATH}\")\n",
    "experiment_name = \"fraud_detection_ieee\"\n",
    "\n",
    "# Create or get experiment\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        artifact_location=str(MLRUNS_PATH / 'artifacts')\n",
    "    )\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow Experiment: {experiment_name} (ID: {experiment_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83362d2e",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "print(\"Loading processed data...\")\n",
    "train_df = pd.read_parquet(PROCESSED_PATH / 'train_processed.parquet')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "# Load feature artifacts\n",
    "with open(FEATURES_PATH / 'feature_artifacts.pkl', 'rb') as f:\n",
    "    feature_artifacts = pickle.load(f)\n",
    "\n",
    "feature_cols = feature_artifacts['feature_cols']\n",
    "print(f\"Number of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a603e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target distribution\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(train_df['isFraud'].value_counts())\n",
    "print(f\"\\nFraud Rate: {train_df['isFraud'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22059e7d",
   "metadata": {},
   "source": [
    "## 2. Time-Based Train/Validation Split\n",
    "\n",
    "### Why Time-Based Split?\n",
    "In production fraud detection:\n",
    "1. Models are trained on historical data\n",
    "2. Models predict on future, unseen transactions\n",
    "3. Random splits would include future transactions in training (data leakage)\n",
    "4. Time-based splits simulate real deployment and reveal concept drift issues\n",
    "\n",
    "We use TransactionDT (time delta) to create a temporal split:\n",
    "- Training: First 80% of transactions (by time)\n",
    "- Validation: Last 20% of transactions (by time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(df, time_col='TransactionDT', train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split data based on time to simulate production conditions.\n",
    "    \n",
    "    This is critical for fraud detection because:\n",
    "    1. Fraud patterns evolve over time (concept drift)\n",
    "    2. Production models always predict on future data\n",
    "    3. Random splits give overly optimistic performance estimates\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with time column\n",
    "        time_col: column containing temporal information\n",
    "        train_ratio: proportion of data for training\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df DataFrames\n",
    "    \"\"\"\n",
    "    # Sort by time\n",
    "    df_sorted = df.sort_values(time_col).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int(len(df_sorted) * train_ratio)\n",
    "    \n",
    "    train = df_sorted.iloc[:split_idx]\n",
    "    val = df_sorted.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Time-based split:\")\n",
    "    print(f\"  Training: {len(train):,} samples ({len(train)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Validation: {len(val):,} samples ({len(val)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Training fraud rate: {train['isFraud'].mean()*100:.2f}%\")\n",
    "    print(f\"  Validation fraud rate: {val['isFraud'].mean()*100:.2f}%\")\n",
    "    \n",
    "    return train, val\n",
    "\n",
    "# Apply time-based split\n",
    "train_data, val_data = time_based_split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Remove any features not in the feature list\n",
    "available_features = [c for c in feature_cols if c in train_data.columns]\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "\n",
    "X_train = train_data[available_features]\n",
    "y_train = train_data['isFraud']\n",
    "X_val = val_data[available_features]\n",
    "y_val = val_data['isFraud']\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1e0c9",
   "metadata": {},
   "source": [
    "## 3. Class Weight Calculation\n",
    "\n",
    "### Why Class Weights Instead of SMOTE?\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)**:\n",
    "- Creates synthetic samples by interpolating between existing minority samples\n",
    "- Risk: Can create unrealistic fraud patterns that don't exist in real data\n",
    "- Memory intensive for large datasets\n",
    "\n",
    "**Class Weights**:\n",
    "- Adjusts the loss function to penalize minority class errors more heavily\n",
    "- No synthetic data creation, preserves data integrity\n",
    "- Computationally efficient\n",
    "- Preferred for fraud detection where false patterns can be dangerous\n",
    "\n",
    "We calculate balanced class weights: `weight = n_samples / (n_classes * n_class_samples)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(y):\n",
    "    \"\"\"\n",
    "    Calculate balanced class weights for imbalanced classification.\n",
    "    \n",
    "    Formula: weight_i = n_samples / (n_classes * n_samples_i)\n",
    "    This gives higher weight to minority class.\n",
    "    \n",
    "    Args:\n",
    "        y: target array\n",
    "    \n",
    "    Returns:\n",
    "        dict mapping class labels to weights\n",
    "    \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Class weights:\")\n",
    "    print(f\"  Class 0 (Legitimate): {class_weights[0]:.4f}\")\n",
    "    print(f\"  Class 1 (Fraud): {class_weights[1]:.4f}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "\n",
    "# For LightGBM scale_pos_weight\n",
    "scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "print(f\"\\nScale pos weight for LightGBM/XGBoost: {scale_pos_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b6536",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics Functions\n",
    "\n",
    "Comprehensive metrics for fraud detection evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred_proba, y_pred=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation for fraud detection.\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual labels\n",
    "        y_pred_proba: predicted probabilities\n",
    "        y_pred: predicted labels (optional, computed from threshold if not provided)\n",
    "        threshold: classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        dict of metrics\n",
    "    \"\"\"\n",
    "    if y_pred is None:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'threshold': threshold\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['true_negatives'] = int(tn)\n",
    "    metrics['false_positives'] = int(fp)\n",
    "    metrics['false_negatives'] = int(fn)\n",
    "    metrics['true_positives'] = int(tp)\n",
    "    \n",
    "    # Business metrics\n",
    "    # False Positive Rate: legitimate transactions flagged as fraud (customer friction)\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    # False Negative Rate: fraud transactions missed (fraud loss)\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    \"\"\"\n",
    "    Find optimal classification threshold based on specified metric.\n",
    "    \n",
    "    For fraud detection:\n",
    "    - Higher threshold = fewer false positives, more false negatives\n",
    "    - Lower threshold = more false positives, fewer false negatives\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual labels\n",
    "        y_pred_proba: predicted probabilities\n",
    "        metric: optimization target ('f1', 'precision', 'recall')\n",
    "    \n",
    "    Returns:\n",
    "        optimal_threshold, best_score\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    \n",
    "    # Calculate F1 for each threshold\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    \n",
    "    if metric == 'f1':\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        return thresholds[best_idx], f1_scores[best_idx]\n",
    "    elif metric == 'precision':\n",
    "        # Find threshold for precision >= 0.5 with best recall\n",
    "        valid_idx = precision >= 0.5\n",
    "        if valid_idx.sum() > 0:\n",
    "            best_idx = np.where(valid_idx)[0][np.argmax(recall[valid_idx])]\n",
    "            return thresholds[min(best_idx, len(thresholds)-1)], precision[best_idx]\n",
    "    \n",
    "    return 0.5, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505cd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_curves(y_true, y_pred_proba, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC and Precision-Recall curves.\n",
    "    \n",
    "    For imbalanced fraud detection:\n",
    "    - ROC AUC can be misleading (looks good even with poor performance)\n",
    "    - PR AUC is more informative for the minority class\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual labels\n",
    "        y_pred_proba: predicted probabilities\n",
    "        model_name: name for title/legend\n",
    "        save_path: optional path to save figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                 label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "    axes[0].set_title(f'{model_name} - ROC Curve', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    baseline = y_true.sum() / len(y_true)\n",
    "    \n",
    "    axes[1].plot(recall, precision, color='green', lw=2,\n",
    "                 label=f'PR curve (AUC = {pr_auc:.4f})')\n",
    "    axes[1].axhline(y=baseline, color='navy', linestyle='--', \n",
    "                    label=f'Baseline = {baseline:.4f}')\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('Recall', fontsize=12)\n",
    "    axes[1].set_ylabel('Precision', fontsize=12)\n",
    "    axes[1].set_title(f'{model_name} - Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136a862",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow Tracking\n",
    "\n",
    "### MLflow Benefits for Fraud Detection:\n",
    "1. **Experiment Tracking**: Compare multiple model configurations\n",
    "2. **Reproducibility**: Log parameters, code versions, data versions\n",
    "3. **Model Registry**: Version and stage models for production\n",
    "4. **Artifact Storage**: Save models, plots, feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model, model_name, X_train, y_train, X_val, y_val, \n",
    "                        params, model_type='sklearn'):\n",
    "    \"\"\"\n",
    "    Train model and log everything to MLflow.\n",
    "    \n",
    "    Args:\n",
    "        model: model instance\n",
    "        model_name: name for logging\n",
    "        X_train, y_train: training data\n",
    "        X_val, y_val: validation data\n",
    "        params: model parameters dict\n",
    "        model_type: 'sklearn', 'lightgbm', or 'xgboost'\n",
    "    \n",
    "    Returns:\n",
    "        trained model, metrics dict, run_id\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name) as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {model_name}\")\n",
    "        print(f\"MLflow Run ID: {run_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param('n_features', X_train.shape[1])\n",
    "        mlflow.log_param('n_train_samples', X_train.shape[0])\n",
    "        mlflow.log_param('n_val_samples', X_val.shape[0])\n",
    "        mlflow.log_param('fraud_rate_train', y_train.mean())\n",
    "        mlflow.log_param('fraud_rate_val', y_val.mean())\n",
    "        \n",
    "        # Train model\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        if model_type == 'lightgbm':\n",
    "            # LightGBM with early stopping\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "            )\n",
    "        elif model_type == 'xgboost':\n",
    "            # XGBoost with early stopping\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        mlflow.log_metric('training_time_seconds', training_time)\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        optimal_threshold, best_f1 = find_optimal_threshold(y_val, y_pred_proba)\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "        \n",
    "        # Evaluate with default and optimal thresholds\n",
    "        metrics_default = evaluate_model(y_val, y_pred_proba, threshold=0.5)\n",
    "        metrics_optimal = evaluate_model(y_val, y_pred_proba, threshold=optimal_threshold)\n",
    "        \n",
    "        # Log metrics\n",
    "        for key, value in metrics_default.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(f'{key}_default', value)\n",
    "        \n",
    "        for key, value in metrics_optimal.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(f'{key}_optimal', value)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nMetrics (threshold=0.5):\")\n",
    "        print(f\"  ROC-AUC: {metrics_default['roc_auc']:.4f}\")\n",
    "        print(f\"  PR-AUC: {metrics_default['pr_auc']:.4f}\")\n",
    "        print(f\"  F1: {metrics_default['f1']:.4f}\")\n",
    "        print(f\"  Precision: {metrics_default['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics_default['recall']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nMetrics (optimal threshold={optimal_threshold:.4f}):\")\n",
    "        print(f\"  F1: {metrics_optimal['f1']:.4f}\")\n",
    "        print(f\"  Precision: {metrics_optimal['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics_optimal['recall']:.4f}\")\n",
    "        \n",
    "        # Generate and log plots\n",
    "        fig = plot_evaluation_curves(\n",
    "            y_val, y_pred_proba, model_name,\n",
    "            save_path=OUTPUT_PATH / 'visuals' / f'{model_name.lower().replace(\" \", \"_\")}_curves.png'\n",
    "        )\n",
    "        mlflow.log_artifact(OUTPUT_PATH / 'visuals' / f'{model_name.lower().replace(\" \", \"_\")}_curves.png')\n",
    "        \n",
    "        # Log model\n",
    "        if model_type == 'lightgbm':\n",
    "            mlflow.lightgbm.log_model(model, 'model')\n",
    "        elif model_type == 'xgboost':\n",
    "            mlflow.xgboost.log_model(model, 'model')\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, 'model')\n",
    "        \n",
    "        # Add model to results\n",
    "        metrics_optimal['model'] = model\n",
    "        metrics_optimal['run_id'] = run_id\n",
    "        metrics_optimal['model_name'] = model_name\n",
    "        metrics_optimal['optimal_threshold'] = optimal_threshold\n",
    "        \n",
    "        return model, metrics_optimal, run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cc96d",
   "metadata": {},
   "source": [
    "### 5.1 LightGBM Model\n",
    "\n",
    "**Why LightGBM for Fraud Detection:**\n",
    "- Handles large datasets efficiently (gradient-based one-side sampling)\n",
    "- Native categorical feature support\n",
    "- Excellent handling of sparse features\n",
    "- Fast training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'num_leaves': 64,\n",
    "    'min_child_samples': 100,\n",
    "    'scale_pos_weight': scale_pos_weight,  # Handle class imbalance\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "lgb_model, lgb_metrics, lgb_run_id = train_and_log_model(\n",
    "    lgb_model, 'LightGBM',\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    lgb_params, model_type='lightgbm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ab20b",
   "metadata": {},
   "source": [
    "### 5.2 XGBoost Model\n",
    "\n",
    "**Why XGBoost:**\n",
    "- Strong regularization (prevents overfitting on noisy fraud data)\n",
    "- Handles missing values natively\n",
    "- Production-proven in many fraud detection systems\n",
    "- GPU acceleration available for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1465970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 100,\n",
    "    'scale_pos_weight': scale_pos_weight,  # Handle class imbalance\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "xgb_model, xgb_metrics, xgb_run_id = train_and_log_model(\n",
    "    xgb_model, 'XGBoost',\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    xgb_params, model_type='xgboost'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884631c",
   "metadata": {},
   "source": [
    "### 5.3 Random Forest Model\n",
    "\n",
    "**Why Random Forest:**\n",
    "- Interpretable feature importance\n",
    "- Robust to outliers and noise\n",
    "- Good baseline for comparison\n",
    "- Handles non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_split': 100,\n",
    "    'min_samples_leaf': 50,\n",
    "    'max_features': 'sqrt',\n",
    "    'class_weight': 'balanced',  # Handle class imbalance\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "rf_model, rf_metrics, rf_run_id = train_and_log_model(\n",
    "    rf_model, 'Random Forest',\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    rf_params, model_type='sklearn'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e2ff9",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'LightGBM',\n",
    "        'ROC-AUC': lgb_metrics['roc_auc'],\n",
    "        'PR-AUC': lgb_metrics['pr_auc'],\n",
    "        'F1': lgb_metrics['f1'],\n",
    "        'Precision': lgb_metrics['precision'],\n",
    "        'Recall': lgb_metrics['recall'],\n",
    "        'Threshold': lgb_metrics['optimal_threshold'],\n",
    "        'Run_ID': lgb_run_id\n",
    "    },\n",
    "    {\n",
    "        'Model': 'XGBoost',\n",
    "        'ROC-AUC': xgb_metrics['roc_auc'],\n",
    "        'PR-AUC': xgb_metrics['pr_auc'],\n",
    "        'F1': xgb_metrics['f1'],\n",
    "        'Precision': xgb_metrics['precision'],\n",
    "        'Recall': xgb_metrics['recall'],\n",
    "        'Threshold': xgb_metrics['optimal_threshold'],\n",
    "        'Run_ID': xgb_run_id\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Random Forest',\n",
    "        'ROC-AUC': rf_metrics['roc_auc'],\n",
    "        'PR-AUC': rf_metrics['pr_auc'],\n",
    "        'F1': rf_metrics['f1'],\n",
    "        'Precision': rf_metrics['precision'],\n",
    "        'Recall': rf_metrics['recall'],\n",
    "        'Threshold': rf_metrics['optimal_threshold'],\n",
    "        'Run_ID': rf_run_id\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of key metrics\n",
    "metrics_to_plot = ['ROC-AUC', 'PR-AUC', 'F1', 'Precision', 'Recall']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(['LightGBM', 'XGBoost', 'Random Forest']):\n",
    "    values = results[results['Model'] == model][metrics_to_plot].values[0]\n",
    "    axes[0].bar(x + i*width, values, width, label=model)\n",
    "\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# PR-AUC comparison (primary metric)\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "axes[1].bar(results['Model'], results['PR-AUC'], color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Precision-Recall AUC', fontsize=12)\n",
    "axes[1].set_title('Primary Metric: PR-AUC Comparison', fontsize=14, fontweight='bold')\n",
    "for i, (model, pr_auc) in enumerate(zip(results['Model'], results['PR-AUC'])):\n",
    "    axes[1].text(i, pr_auc + 0.01, f'{pr_auc:.4f}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'visuals' / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on PR-AUC (primary metric for imbalanced classification)\n",
    "best_model_idx = results['PR-AUC'].idxmax()\n",
    "best_model_name = results.loc[best_model_idx, 'Model']\n",
    "best_model_pr_auc = results.loc[best_model_idx, 'PR-AUC']\n",
    "best_run_id = results.loc[best_model_idx, 'Run_ID']\n",
    "\n",
    "# Get the actual best model object\n",
    "if best_model_name == 'LightGBM':\n",
    "    best_model = lgb_model\n",
    "    best_metrics = lgb_metrics\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_model = xgb_model\n",
    "    best_metrics = xgb_metrics\n",
    "else:\n",
    "    best_model = rf_model\n",
    "    best_metrics = rf_metrics\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"PR-AUC: {best_model_pr_auc:.4f}\")\n",
    "print(f\"Run ID: {best_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00ccf8",
   "metadata": {},
   "source": [
    "## 7. Save Best Model\n",
    "\n",
    "Save the best model with all artifacts needed for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a989fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model locally\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_model_name,\n",
    "    'optimal_threshold': best_metrics['optimal_threshold'],\n",
    "    'metrics': best_metrics,\n",
    "    'feature_cols': available_features,\n",
    "    'run_id': best_run_id,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(MODELS_PATH / 'best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(f\"Best model saved to: {MODELS_PATH / 'best_model.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d665cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "results.to_csv(OUTPUT_PATH / 'metrics' / 'model_comparison.csv', index=False)\n",
    "print(f\"Results saved to: {OUTPUT_PATH / 'metrics' / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed556821",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Analysis\n",
    "\n",
    "Perform stratified cross-validation on the best model to estimate performance variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d50228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold cross-validation\n",
    "# Note: In production, we'd use time-series cross-validation\n",
    "# Here we demonstrate stratified CV for completeness\n",
    "\n",
    "print(\"Performing 5-fold stratified cross-validation on best model...\")\n",
    "\n",
    "# Combine train and val for CV\n",
    "X_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_full = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Use a fresh model instance for CV\n",
    "if best_model_name == 'LightGBM':\n",
    "    cv_model = lgb.LGBMClassifier(**lgb_params)\n",
    "elif best_model_name == 'XGBoost':\n",
    "    cv_model = xgb.XGBClassifier(**{k:v for k,v in xgb_params.items() if k != 'early_stopping_rounds'})\n",
    "else:\n",
    "    cv_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(cv_model, X_full, y_full, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"\\nCross-Validation ROC-AUC Scores:\")\n",
    "print(f\"  Scores: {cv_scores}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eab90e",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Model Selection**: Selected best model based on PR-AUC (most appropriate for imbalanced fraud detection)\n",
    "\n",
    "2. **Class Imbalance Handling**: Used class weights to adjust for ~3.5% fraud rate\n",
    "\n",
    "3. **Time-Based Split**: Simulates production conditions and reveals temporal patterns\n",
    "\n",
    "4. **MLflow Tracking**: All experiments logged for reproducibility and comparison\n",
    "\n",
    "### Business Implications\n",
    "\n",
    "- **Precision vs Recall Trade-off**: \n",
    "  - Higher threshold = fewer false positives (better customer experience)\n",
    "  - Lower threshold = fewer false negatives (catch more fraud)\n",
    "  - Optimal threshold depends on business costs of each error type\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Proceed to model interpretation notebook for explainability\n",
    "2. Deploy model using MLflow model registry\n",
    "3. Implement monitoring for concept drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e687a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODELING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"PR-AUC: {best_model_pr_auc:.4f}\")\n",
    "print(f\"Optimal Threshold: {best_metrics['optimal_threshold']:.4f}\")\n",
    "print(f\"\\nModel artifacts saved to: {MODELS_PATH}\")\n",
    "print(f\"MLflow experiments at: {MLRUNS_PATH}\")\n",
    "print(f\"\\nTo view MLflow UI, run: mlflow ui --backend-store-uri {MLRUNS_PATH}\")\n",
    "print(\"\\nNext steps: Proceed to 04_model_interpretation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
